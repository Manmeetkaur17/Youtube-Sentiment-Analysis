{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manmeetkaur17/Youtube-Sentiment-Analysis/blob/main/youTubeSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U1LDtIB122a3"
      },
      "outputs": [],
      "source": [
        "pip install google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3aktAISgrBT"
      },
      "outputs": [],
      "source": [
        "!pip uninstall fasttext-numpy2\n",
        "!pip install fasttext\n",
        "# !pip install pandas\n",
        "# !pip install deep-translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-enlOqp5i0f"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "api_key = \"AIzaSyBN8WxK4mDlU8f4_iJn6_8atBYe8GgVqsw\"\n",
        "video_url = \"https://www.youtube.com/watch?v=gq2bbDmSokU\"\n",
        "\n",
        "# Use the extract_video_id function to get the video ID\n",
        "def extract_video_id(url):\n",
        "    pattern = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "video_id = extract_video_id(video_url)\n",
        "\n",
        "def get_youtube_comments(video_id, api_key, max_results=100):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    comments = []\n",
        "    next_page_token = None\n",
        "\n",
        "    while len(comments) < max_results:\n",
        "        request = youtube.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=min(100, max_results - len(comments)),\n",
        "            textFormat=\"plainText\",\n",
        "            pageToken=next_page_token\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        next_page_token = response.get('nextPageToken')\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "comments = get_youtube_comments(video_id, api_key, max_results=50)\n",
        "df = pd.DataFrame(comments, columns=[\"Original Comment\"])\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bznRJodPbzV7"
      },
      "outputs": [],
      "source": [
        "#download the pre-trained language detection model\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "BbdMP0b5L3EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, DetectorFactory\n",
        "import pandas as pd\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(str(text).strip())\n",
        "    except Exception as e:\n",
        "        return f\"error: {e}\"\n",
        "\n",
        "df['Language'] = df['Original Comment'].apply(detect_language)\n",
        "print(df[['Original Comment', 'Language']].head())\n"
      ],
      "metadata": {
        "id": "vuBcN_HLL65B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "id": "2sjLr1ObL1Pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvhUDNxhlIN4"
      },
      "outputs": [],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "def translate_to_english(text, lang_code):\n",
        "    try:\n",
        "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
        "    except Exception as e:\n",
        "        print(\"Translation error:\", e)\n",
        "        return text\n",
        "\n",
        "df['Translated Comment'] = df.apply(lambda row: translate_to_english(row['Original Comment'], row['Language']), axis=1)\n",
        "print(df[['Original Comment', 'Language', 'Translated Comment']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZizsiZF4MWsu"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"youtube_comments_translated.csv\", index=False)\n",
        "print(\"Saved as youtube_comments_translated.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUv0VB8SMpXZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkJYC-4RM-c"
      },
      "source": [
        "## **Comments Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdlgsCRbRULF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRw0qwzdRj3p"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load your translated comments (assuming df['Translated Comment'] exists)\n",
        "# Join all comments into a single paragraph (T5 expects a \"document\")\n",
        "# Filter out None values before joining\n",
        "\n",
        "if 'Translated Comment' in df.columns:\n",
        "    all_comments = ' '.join([comment for comment in df['Translated Comment'].tolist() if comment is not None])\n",
        "else:\n",
        "    all_comments = ' '.join([comment for comment in df['Original Comment'].tolist() if comment is not None])\n",
        "    print(\"Warning: 'Translated Comment' column not found. Using 'Original Comment' for summarization.\")\n",
        "\n",
        "\n",
        "# Limit input size (T5 has max token limit ~512â€“1024)\n",
        "# Optional: truncate or split if needed\n",
        "if len(all_comments.split()) > 500:\n",
        "    all_comments = ' '.join(all_comments.split()[:500])  # use first 500 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUn_fjfoRuJ_"
      },
      "outputs": [],
      "source": [
        "all_comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaxTM4yrS1Ct"
      },
      "outputs": [],
      "source": [
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Prepare input text for summarization\n",
        "input_text = \"summarize: \" + all_comments\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ngo2JEnTAXO"
      },
      "outputs": [],
      "source": [
        "# Generate the summary\n",
        "summary_ids = model.generate(input_ids, num_beams=4, min_length=30, max_length=100, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nðŸ“Œ Summary of YouTube Comments:\")\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eAVRwzcTEkv"
      },
      "outputs": [],
      "source": [
        "video_url = \"https://www.youtube.com/watch?v=gq2bbDmSokU\"\n",
        "pattern = r\"(?:v=|)([0-9A-Za-z_-]{11}).*\"\n",
        "match = re.search(pattern, video_url)\n",
        "print(match)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match"
      ],
      "metadata": {
        "id": "EBZIR_YQPPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pre-trained sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        # The model returns a list of dictionaries, we take the first one\n",
        "        result = sentiment_analyzer(text)[0]\n",
        "        return result['label']\n",
        "    except Exception as e:\n",
        "        print(f\"Sentiment analysis error: {e}\")\n",
        "        return \"error\"\n",
        "\n",
        "# Apply sentiment analysis to the translated comments\n",
        "df['Sentiment'] = df['Translated Comment'].apply(get_sentiment)\n",
        "\n",
        "# Display the DataFrame with the new Sentiment column\n",
        "print(df[['Original Comment', 'Translated Comment', 'Sentiment']].head())"
      ],
      "metadata": {
        "id": "iWlN4ez3jMbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qufZr5dMMfgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnD5Er+HyW4V9SqKLHWUho",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}